{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6104837,"sourceType":"datasetVersion","datasetId":3497143}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 0 â€” Install dependencies (Unsloth-safe + latest transformers)\n\n!pip uninstall -y transformers tokenizers peft -qq\n!pip install --quiet --upgrade transformers tokenizers peft unsloth\n!pip install --quiet datasets evaluate accelerate bitsandbytes safetensors protobuf==3.20.*","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T03:58:13.111255Z","iopub.execute_input":"2026-01-03T03:58:13.111929Z","iopub.status.idle":"2026-01-03T04:02:11.485052Z","shell.execute_reply.started":"2026-01-03T03:58:13.111900Z","shell.execute_reply":"2026-01-03T04:02:11.484023Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m378.2/378.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.9.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1 â€” Imports & Global Config\n\nimport os, sys, json, math, time\nimport torch\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom abc import ABC, abstractmethod\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Clears unused GPU memory cache\ntorch.cuda.empty_cache()\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA devices:\", torch.cuda.device_count())\nfor i in range(torch.cuda.device_count()):\n    print(i, torch.cuda.get_device_name(i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:02:11.486956Z","iopub.execute_input":"2026-01-03T04:02:11.487490Z","iopub.status.idle":"2026-01-03T04:02:13.776363Z","shell.execute_reply.started":"2026-01-03T04:02:11.487465Z","shell.execute_reply":"2026-01-03T04:02:13.775661Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nCUDA devices: 2\n0 Tesla T4\n1 Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 2 â€” HuggingFace Login\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nhf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:02:13.777131Z","iopub.execute_input":"2026-01-03T04:02:13.777723Z","iopub.status.idle":"2026-01-03T04:02:14.905208Z","shell.execute_reply.started":"2026-01-03T04:02:13.777703Z","shell.execute_reply":"2026-01-03T04:02:14.904593Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# CELL 3 â€” Load Model & Tokenizer\n\nfrom unsloth import FastLanguageModel\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = 4096,      # Maximum number of tokens the model can process in one sequence\n    dtype = torch.float16,\n    load_in_4bit = True,     # Loads model weights in 4-bit quantization\n    token = hf_token,\n    device_map = \"auto\",          # auto-distribute across T4 * 2\n    offload_folder = \"/kaggle/working/offload\",\n    use_gradient_checkpointing = True  #  standard PyTorch checkpointing instead of Unsloth patch\n)\n\n# Set the padding token to be the same as the end-of-sequence token\n\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:02:14.905986Z","iopub.execute_input":"2026-01-03T04:02:14.906207Z","iopub.status.idle":"2026-01-03T04:03:30.532446Z","shell.execute_reply.started":"2026-01-03T04:02:14.906182Z","shell.execute_reply":"2026-01-03T04:03:30.531693Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-01-03 04:02:18.024659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767412938.196472      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767412938.247439      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.12.10: Fast Llama patching. Transformers: 4.57.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29f75a1c10643ae947b7130094b414b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99cadfe2151e4623b4a6b9986b801916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5409041a004c648b2ef897ad3a1aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc6321a30fd64acc80ae2491d54a153b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c9dabb91ab423aa905a4d4ee2b3600"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# CELL 4 â€” Dataset Processor\n\nfrom datasets import Dataset\nimport pandas as pd\n\nclass DatasetProcessor:\n    def __init__(self, csv_path, tokenizer, max_length=4096, max_train_samples=None):\n        self.csv_path = csv_path\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.max_train_samples = max_train_samples\n\n    def load(self):\n        df = pd.read_csv(self.csv_path)\n        df = df[df[\"Questions\"].notna() & df[\"Answers\"].notna()]\n        if self.max_train_samples:\n            df = df.sample(min(len(df), self.max_train_samples), random_state=42)\n        return Dataset.from_pandas(df)\n\n    def tokenize(self, dataset):\n        def tokenize_fn(example):\n            prompt = (\n                f\"à¦¬à¦¿à¦·à¦¯à¦¼: {example['Topics']}\\n\"\n                f\"à¦¶à¦¿à¦°à§‹à¦¨à¦¾à¦®: {example['Question-Title']}\\n\"\n                f\"à¦ªà§à¦°à¦¶à§à¦¨: {example['Questions']}\\n\\n\"\n                f\"à¦¸à¦¹à¦¾à¦¨à§à¦­à§‚à¦¤à¦¿à¦¶à§€à¦² à¦‰à¦¤à§à¦¤à¦°:\"\n            )\n            answer = example[\"Answers\"]\n            full_text = prompt + \" \" + answer\n\n            #  Truncate sequences to model's max_length\n            tokenized = self.tokenizer(\n                full_text,\n                truncation=True,     # Prevents CUDA memory overflow\n                padding=False,\n                max_length=self.max_length,\n            )\n            \n            # Tokenize prompt separately (without special tokens)\n            prompt_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n            labels = tokenized[\"input_ids\"].copy()\n            labels[:len(prompt_ids)] = [-100] * len(prompt_ids)  # Mask prompt tokens\n\n            tokenized[\"labels\"] = labels\n\n            #  Required for evaluation\n            tokenized[\"eval_prompt\"] = prompt\n            tokenized[\"eval_answer\"] = answer\n\n            return tokenized\n            \n        # Apply tokenize_fn to entire dataset\n        return dataset.map(tokenize_fn, remove_columns=dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:30.534323Z","iopub.execute_input":"2026-01-03T04:03:30.534622Z","iopub.status.idle":"2026-01-03T04:03:30.543422Z","shell.execute_reply.started":"2026-01-03T04:03:30.534602Z","shell.execute_reply":"2026-01-03T04:03:30.542587Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# CELL 5 â€” Strategy Pattern (Unsloth, LoRA)\n\n#  Abstract base class for fine-tuning strategy\nclass FineTuneStrategy(ABC):\n    @abstractmethod\n    def apply(self, model):\n        \n        \"\"\"Apply the fine-tuning strategy to the model\"\"\"\n        pass\n\n\n# ğŸ”¹ LoRA Strategy (Optional PEFT)\nclass LoRAStrategy(FineTuneStrategy):\n    def __init__(self, r=16, alpha=32, dropout=0.05):\n        self.r = r\n        self.alpha = alpha\n        self.dropout = dropout\n        self.target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n    def apply(self, model):\n        \"\"\"\n        Apply LoRA-based parameter-efficient fine-tuning.\n        \"\"\"\n        model = FastLanguageModel.get_peft_model(\n            model,\n            r=self.r,\n            target_modules=self.target_modules,\n            lora_alpha=self.alpha,\n            lora_dropout=self.dropout,\n            bias=\"none\",\n            use_gradient_checkpointing=True,  # standard PyTorch checkpointing\n        )\n        lora_config = {\n            \"strategy\": \"LoRA\",\n            \"r\": self.r,\n            \"alpha\": self.alpha,\n            \"dropout\": self.dropout,\n            \"target_modules\": self.target_modules\n        }\n        return model, lora_config\n\n#  ACTIVE STRATEGY â€” Unsloth (QUALITY-OPTIMIZED)\nclass UnslothStrategy(FineTuneStrategy):\n    def apply(self, model):\n        \n        # Attach LoRA adapters to the base LLaMA model\n        model = FastLanguageModel.get_peft_model(\n            model,\n            r = 16,   # Rank controls LoRA capacity \n            target_modules = [\n                \"q_proj\",   # Query projection (attention focus)\n                \"k_proj\",   # Key projection (context matching)\n                \"v_proj\",   # Value projection (information flow)\n                \"o_proj\",   #  Final Output projection\n            ],\n            lora_alpha = 32,     # Alpha scales LoRA updates\n            lora_dropout = 0.05,  # Prevents overfitting\n            bias = \"none\",  # Bias parameters are not trained\n            use_gradient_checkpointing = \"unsloth\",  # memory-efficient\n        )\n        \n        # Required for reproducibility & reporting\n        lora_config = {\n            \"strategy\": \"Unsloth\",\n            \"r\": 16,\n            \"alpha\": 32,\n            \"dropout\": 0.05,\n            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        }\n\n        return model, lora_config\n\n\n#  Function to dynamically select strategy\ndef select_strategy(strategy_name=\"unsloth\"):\n    \"\"\"\n    Returns the strategy instance based on the name.\n    Options: \"unsloth\", \"lora\"\n    \"\"\"\n    if strategy_name.lower() == \"lora\":\n        return LoRAStrategy()\n    else:\n        return UnslothStrategy()\n\n\n# Dynamically choose strategy\nchosen_strategy_name = \"unsloth\"  # change to \"lora\" if needed\nstrategy = select_strategy(chosen_strategy_name)\nmodel, lora_config = strategy.apply(model)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:30.544350Z","iopub.execute_input":"2026-01-03T04:03:30.545062Z","iopub.status.idle":"2026-01-03T04:03:39.630970Z","shell.execute_reply.started":"2026-01-03T04:03:30.545043Z","shell.execute_reply":"2026-01-03T04:03:39.630152Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2025.12.10 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# CELL 6 â€” Data Collator\n\nfrom torch.nn.utils.rnn import pad_sequence\n\n# This class will be called by HuggingFace Trainer for all training batch\n\nclass LLMDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n\n        # Convert input_ids to PyTorch tensors\n        input_ids = [torch.tensor(x[\"input_ids\"]) for x in batch]\n\n        # Convert labels to PyTorch tensors\n        labels = [torch.tensor(x[\"labels\"]) for x in batch]\n\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n\n        # Create attention mask\n        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"attention_mask\": attention_mask,\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:39.631843Z","iopub.execute_input":"2026-01-03T04:03:39.632055Z","iopub.status.idle":"2026-01-03T04:03:39.637628Z","shell.execute_reply.started":"2026-01-03T04:03:39.632038Z","shell.execute_reply":"2026-01-03T04:03:39.636939Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# CELL 7 â€” LLAMAFineTuner\n\nfrom transformers import Trainer, TrainingArguments\n\nclass LLAMAFineTuner:\n    def __init__(self, model, tokenizer, train_ds):\n        self.model = model        # Store model (already wrapped with Unsloth LoRA)\n        self.tokenizer = tokenizer      # Store tokenizer (needed by Trainer for padding & saving)\n        self.train_ds = train_ds        # Store training dataset (tokenized HuggingFace Dataset)\n        self.checkpoint_dir = \"/kaggle/working/checkpoints\"\n\n    # This method sets up training configuration and launches training\n    def train(self):\n        args = TrainingArguments(\n                output_dir=self.checkpoint_dir,\n                per_device_train_batch_size=1,  # Batch size per GPU\n                gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n                num_train_epochs=2,  # Number of full passes over the dataset\n                learning_rate=2e-4,  \n                fp16=True,  # Enables half-precision training\n                save_strategy=\"steps\",  # Save model checkpoints based on step count\n                save_steps=600,  \n                logging_steps=500,\n                report_to=\"none\",  # Disable logging to external services\n                optim=\"adamw_torch\",  # Uses PyTorch-native AdamW\n                dataloader_pin_memory=False,  # Prevents CUDA memory fragmentation on Kaggle\n            )\n\n\n        trainer = Trainer(\n            model=self.model,\n            args=args,\n            train_dataset=self.train_ds,\n            tokenizer=self.tokenizer,\n            data_collator=LLMDataCollator(self.tokenizer),\n        )\n\n        return trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:39.638377Z","iopub.execute_input":"2026-01-03T04:03:39.638674Z","iopub.status.idle":"2026-01-03T04:03:39.653236Z","shell.execute_reply.started":"2026-01-03T04:03:39.638647Z","shell.execute_reply":"2026-01-03T04:03:39.652589Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# CELL 8 â€” Evaluator\n\nimport evaluate\nimport math\nimport torch\n\nclass Evaluator:\n    def __init__(self, tokenizer, model):\n        self.bleu = evaluate.load(\"bleu\")\n        self.rouge = evaluate.load(\"rouge\")\n        self.tokenizer = tokenizer\n        self.model = model\n\n    def perplexity(self, loss):\n        \"\"\"Compute perplexity from training loss\"\"\"\n        return math.exp(loss)\n\n    def text_metrics(self, dataset, samples=50):\n        \"\"\"\n        Compute BLEU and ROUGE metrics for generated text.\n        Ensures prompt is removed from prediction before comparison.\n        \"\"\"\n        self.model.eval()\n        preds, refs = [], []\n\n        # Limit to 'samples' number of examples\n        for i in range(min(samples, len(dataset))):\n            item = dataset[i]\n\n            # Encode the prompt only\n            inputs = self.tokenizer(\n                item[\"eval_prompt\"],\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length\n            ).to(self.model.device)\n\n            with torch.no_grad():\n                out = self.model.generate(\n                    **inputs,\n                    max_new_tokens=150,\n                    do_sample=False  # deterministic generation for evaluation\n                )\n\n            # Decode output\n            pred_text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n\n            #  Strip prompt from generated text\n            if pred_text.startswith(item[\"eval_prompt\"]):\n                pred_text = pred_text[len(item[\"eval_prompt\"]):].strip()\n\n            preds.append(pred_text)\n            refs.append([item[\"eval_answer\"].strip()])\n\n        # Compute BLEU (strict n-gram overlap)\n        bleu_score = self.bleu.compute(predictions=preds, references=refs)[\"bleu\"]\n\n        # Compute ROUGE\n        rouge_scores = self.rouge.compute(predictions=preds, references=refs)\n\n        return {\n            \"bleu\": bleu_score,\n            \"rouge\": rouge_scores\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:39.654134Z","iopub.execute_input":"2026-01-03T04:03:39.654426Z","iopub.status.idle":"2026-01-03T04:03:39.729523Z","shell.execute_reply.started":"2026-01-03T04:03:39.654410Z","shell.execute_reply":"2026-01-03T04:03:39.728995Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# CELL 9 â€” Train + Evaluate + Metrics Table\n\nCSV_PATH = \"/kaggle/input/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv\"\n\nprocessor = DatasetProcessor(CSV_PATH, tokenizer, max_train_samples=6000)\ndataset = processor.tokenize(processor.load())\n\ntuner = LLAMAFineTuner(model, tokenizer, dataset)\ntrain_output = tuner.train()\n\n# ğŸ”¹ Evaluation\ntorch.cuda.empty_cache()\nevaluator = Evaluator(tokenizer, model)\ntext_metrics = evaluator.text_metrics(dataset)\n\nmetrics = {\n    \"perplexity\": evaluator.perplexity(train_output.training_loss),\n    \"bleu\": text_metrics[\"bleu\"],\n    \"rouge-1\": text_metrics[\"rouge\"][\"rouge1\"], # ROUGE-1: unigram overlap\n    \"rouge-2\": text_metrics[\"rouge\"][\"rouge2\"], # ROUGE-2: bigram overlap\n    \"rouge-l\": text_metrics[\"rouge\"][\"rougeL\"], # ROUGE-L: longest common subsequence\n} \n\n#  Evaluation table\neval_table = pd.DataFrame(\n    metrics.items(),\n    columns=[\"Metric\", \"Score\"]\n)\n\nprint(\"\\n=== Evaluation Metrics ===\")\ndisplay(eval_table)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:03:39.730154Z","iopub.execute_input":"2026-01-03T04:03:39.730406Z","iopub.status.idle":"2026-01-03T09:07:25.524457Z","shell.execute_reply.started":"2026-01-03T04:03:39.730392Z","shell.execute_reply":"2026-01-03T09:07:25.523834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"234a3cb565594983b12404055f97bf51"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/2196880445.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n  trainer = Trainer(\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 6,000 | Num Epochs = 2 | Total steps = 3,000\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n \"-____-\"     Trainable parameters = 13,631,488 of 8,043,892,736 (0.17% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 4:55:07, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.736000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.707200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.685200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.646000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.644500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.645300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf44ad3751b54d8287f5cb47fc396a67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6767bbad81504267b27afedd91b5ba5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205783040a9e4e86bae6031034831734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a0bdf0a12a4d849f76f48079903b4d"}},"metadata":{}},{"name":"stdout","text":"\n=== Evaluation Metrics ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       Metric     Score\n0  perplexity  1.968716\n1        bleu  0.008530\n2     rouge-1  0.000000\n3     rouge-2  0.000000\n4     rouge-l  0.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>perplexity</td>\n      <td>1.968716</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bleu</td>\n      <td>0.008530</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rouge-1</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>rouge-2</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rouge-l</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# CELL 10 â€” Save Experiment Logs\n\nexperiment_log = {\n    \"id\": 1,\n    \"model_name\": MODEL_NAME,\n    \"lora_config\": lora_config,\n    \"train_loss\": train_output.training_loss,\n    \"metrics\": metrics,\n    \"timestamp\": datetime.now().isoformat()\n}\n\nPath(\"/kaggle/working/LLAMAExperiments.json\").write_text(\n    json.dumps(experiment_log, indent=2, ensure_ascii=False)\n)\n","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2026-01-03T09:07:25.525395Z","iopub.execute_input":"2026-01-03T09:07:25.525807Z","iopub.status.idle":"2026-01-03T09:07:25.532009Z","shell.execute_reply.started":"2026-01-03T09:07:25.525760Z","shell.execute_reply":"2026-01-03T09:07:25.531360Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"491"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# CELL 11 â€” Save Final Model\nSAVE_DIR = \"/kaggle/working/llama_unsloth_final\"\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T09:07:25.532643Z","iopub.execute_input":"2026-01-03T09:07:25.532970Z","iopub.status.idle":"2026-01-03T09:07:27.155334Z","shell.execute_reply.started":"2026-01-03T09:07:25.532946Z","shell.execute_reply":"2026-01-03T09:07:27.154683Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/llama_unsloth_final/tokenizer_config.json',\n '/kaggle/working/llama_unsloth_final/special_tokens_map.json',\n '/kaggle/working/llama_unsloth_final/chat_template.jinja',\n '/kaggle/working/llama_unsloth_final/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# CELL 12 â€” Human Evaluation Generation\nprompts = [\n    \"à¦†à¦®à¦¿ à¦–à§à¦¬ à¦¦à§à¦ƒà¦–à¦¿à¦¤, à¦•à§‡à¦‰ à¦†à¦®à¦¾à¦•à§‡ à¦¬à§‹à¦à§‡ à¦¨à¦¾à¥¤\",\n    \"à¦†à¦®à¦¿ à¦†à¦œ à¦–à§à¦¬ à¦–à§à¦¶à¦¿, à¦•à¦¾à¦°à¦£ à¦†à¦®à¦¾à¦° à¦ªà¦°à¦¿à¦¬à¦¾à¦° à¦†à¦®à¦¾à¦•à§‡ à¦¸à¦®à¦°à§à¦¥à¦¨ à¦•à¦°à§‡à¥¤\"\n]\n\nresponses = []\n\nfor i, p in enumerate(prompts):\n    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n    out = model.generate(**inputs, max_new_tokens=200)\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n\n    responses.append({\n        \"id\": i + 1,\n        \"experiment_id\": 1,\n        \"input_text\": p,\n        \"response_text\": text,\n        \"timestamp\": datetime.now().isoformat(),\n        \"human_score\": None\n    })\n\nPath(\"/kaggle/working/GeneratedResponses.json\").write_text(\n    json.dumps(responses, indent=2, ensure_ascii=False)\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T09:07:27.156157Z","iopub.execute_input":"2026-01-03T09:07:27.156514Z","iopub.status.idle":"2026-01-03T09:07:53.395572Z","shell.execute_reply.started":"2026-01-03T09:07:27.156470Z","shell.execute_reply":"2026-01-03T09:07:53.394674Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"857"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# CELL 13 â€” Sample Model Response on Test Prompt (Printed Output)\n\ntest_prompt = \"à¦†à¦®à¦¿ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦›à¦¿ à¦•à¦¿à¦¨à§à¦¤à§ à¦•à¦¿à¦›à§à¦¤à§‡à¦‡ à¦¸à¦«à¦² à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿ à¦¨à¦¾, à¦†à¦¤à§à¦®à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸ à¦¹à¦¾à¦°à¦¿à¦¯à¦¼à§‡ à¦«à§‡à¦²à§‡à¦›à¦¿à¥¤\"\n\nmodel.eval()\n\ninputs = tokenizer(\n    test_prompt,\n    return_tensors=\"pt\"\n).to(model.device)\n\nwith torch.no_grad():\n    output = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )\n\n# Extract only newly generated tokens\ngenerated_ids = output[0][inputs[\"input_ids\"].shape[1]:]\nresponse_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\nprint(\"\\n================ SAMPLE MODEL RESPONSE ================\\n\")\nprint(\"ğŸŸ¢ Test Prompt:\")\nprint(test_prompt)\n\nprint(\"\\nğŸŸ¢ Model Response:\")\nprint(response_text)\nprint(\"\\n======================================================\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T09:07:53.397621Z","iopub.execute_input":"2026-01-03T09:07:53.397925Z","iopub.status.idle":"2026-01-03T09:08:06.488185Z","shell.execute_reply.started":"2026-01-03T09:07:53.397907Z","shell.execute_reply":"2026-01-03T09:08:06.487531Z"}},"outputs":[{"name":"stdout","text":"\n================ SAMPLE MODEL RESPONSE ================\n\nğŸŸ¢ Test Prompt:\nà¦†à¦®à¦¿ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦›à¦¿ à¦•à¦¿à¦¨à§à¦¤à§ à¦•à¦¿à¦›à§à¦¤à§‡à¦‡ à¦¸à¦«à¦² à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿ à¦¨à¦¾, à¦†à¦¤à§à¦®à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸ à¦¹à¦¾à¦°à¦¿à¦¯à¦¼à§‡ à¦«à§‡à¦²à§‡à¦›à¦¿à¥¤\n\nğŸŸ¢ Model Response:\n à¦†à¦®à¦¿ à¦†à¦ªà¦¨à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦ªà§à¦°à§‹à¦ªà§à¦°à¦¿ à¦à¦•à¦®à¦¤! à¦†à¦®à¦¿ à¦ªà§à¦°à§‹à¦ªà§à¦°à¦¿ à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤ à¦¨à¦‡à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦†à¦®à¦¿ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦¯à§‡ à¦†à¦®à¦¿ à¦¯à¦¦à¦¿ à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤ à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à¦¿, à¦¤à¦¾à¦¹à¦²à§‡ à¦à¦Ÿà¦¿ à¦à¦–à¦¨à¦“ à¦à¦•à¦Ÿà¦¿ à¦šà¦¾à¦²à¦¾à¦• à¦¬à¦¾ à¦…à¦¤à§à¦¯à¦¨à§à¦¤ à¦¬à¦¿à¦ªà¦œà§à¦œà¦¨à¦• à¦…à¦­à¦¿à¦œà§à¦à¦¤à¦¾ à¦¹ï¿½\n\n======================================================\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}